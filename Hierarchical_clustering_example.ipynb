{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNSb7Y9Wx5y9"
   },
   "source": [
    "# Examples: Hierarchical clustering\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this train, we will cover hierarchical clustering, focusing on agglomerative clustering. We will explore the algorithm, interpret dendrograms, implement it using sklearn and SciPy, and determine the optimal number of clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "By the end of this train, you should be able to:\n",
    "\n",
    "* Understand hierarchical clustering.\n",
    "* Interpret dendrograms.\n",
    "* Implement agglomerative clustering in sklearn and SciPy.\n",
    "* Determine the optimal number of clusters from a dendrogram.\n",
    "* Explore the advantages and disadvantages of agglomerative hierarchical clustering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this train, we discuss **Hierarchical clustering** - a clustering method where data is arranged in a tree-like structure such that parent clusters contain smaller child clusters, which also have their own child clusters and so on. There are two main types of hierarchical clustering algorithms:\n",
    "1. Divisive hierarchical clustering - a top-down clustering approach that starts with a large single cluster and iteratively divides it into smaller and smaller clusters until each cluster contains a single data point.\n",
    "2. Agglomerative hierarchical clustering - a bottom-up approach that starts with many \"single data point clusters\" and iteratively merges them into fewer and fewer clusters until all data points belong to the same cluster.\n",
    "\n",
    "These two methods are similar and rely on the same concepts. As such, we will only cover agglomerative hierarchical clustering in this train.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative hierarchical clustering\n",
    "The hierarchical clustering algorithm operates by defining a **\"dissimilarity\" measure** (most commonly taken as the Euclidean distance) between pairs of data points. Each data point initially begins as its own cluster and the two \"most similar\" clusters join to form a new cluster and this is repeated iteratively until all the data points belong to the same cluster. Once this process is complete, a suitable number of clusters can be determined by the resulting **dendrogram**. The advantage of this algorithm is that we do not need to decide on the number of clusters K beforehand, as is the case with the k-means algorithm.   \n",
    "\n",
    "**The basic algorithm of Agglomerative clustering is as follows**:\n",
    "1. Let each data point be a cluster\n",
    "2. Compute the distance matrix\n",
    "3. Determine the linkage criteria to merge the clusters\n",
    "4. Repeat: Merge the two closest clusters and update the distance matrix\n",
    "5. Continue until only a single cluster remains\n",
    "\n",
    "The algorithm can be summarised using the following pseudocode:\n",
    "\n",
    "<pre>\n",
    "*start with each observation in its own cluster\n",
    "\n",
    "n_clusters = n_observations\n",
    "\n",
    "while n_clusters > 1:\n",
    "      \n",
    "      *compare all pairwise distances between clusters\n",
    "      \n",
    "      *find the closest (i.e. least dissimilar) two clusters and merge them\n",
    "      \n",
    "      n_clusters -= 1\n",
    "</pre>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing which two clusters to merge\n",
    "\n",
    "Hierarchical clustering methods heavily rely on how the dissimilarity measure is computed. Furthermore, we might get completely different results depending on how we choose to compute this dissimilarity. The only way we can adjust the dissimilarity is through the following hierarchical clustering hyperparameters:\n",
    "  1. Distance metric; and\n",
    "  2. Linkage method.\n",
    "  \n",
    "Each hyperparameter will determine the criteria on which we base our decision to merge any two clusters.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Distance metric\n",
    "There is a wide variety of distance metrics from which to choose. The most commonly used metric is the Euclidean distance, the straight line distance between two points. Other standard metrics include the Chebyshev and Manhattan distances. Below, the grey values in each cell represent the distance from the cell containing the blue dot to that cell.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-distances.png?raw=true\" alt=\"Dendrogram\" style=\"width: 80%;\"/>  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an excellent explanation of these three metrics, taken from [this website](https://lyfat.wordpress.com/2012/05/22/euclidean-vs-chebyshev-vs-manhattan-distance/):   \n",
    "\n",
    "> \"In chess, the distance between squares on the chessboard for rooks is measured in Manhattan distance; kings and queens use Chebyshev distance, and bishops use the Manhattan distance (between squares of the same colour) on the chessboard rotated 45 degrees, i.e., with its diagonals as coordinate axes. To reach from one square to another, only kings require the number of moves equal to the distance; rooks, queens and bishops require one or two moves (on an empty board, and assuming that the move is possible at all in the bishop’s case).\"\n",
    "\n",
    "There are still many more distance metrics from which you can choose. We will be using the `SciPy` library for clustering, so you should take a look at [their docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html#scipy.spatial.distance.pdist) for a list of all available distance metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linkage method\n",
    "The linkage method determines _how_ we measure the chosen distance between two clusters.   \n",
    "\n",
    "Take a moment to consider how you would measure the distance between two clusters with multiple points. Would you choose to measure the distance between the closest two points? The furthest? The average of all distances?   \n",
    "\n",
    "All of these are valid options! The diagram below illustrates the three most common linkage methods:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-linkages.png?raw=true\" alt=\"Dendrogram\" style=\"width: 70%;\"/>  \n",
    " \n",
    "\n",
    "  1. **Single linkage:** The distance between the two closest points;\n",
    "  2. **Complete linkage:** The distance between the two furthest points;\n",
    "  3. **Average linkage:** The average of all pairwise distances between points.\n",
    "  \n",
    "Again, there are still a few more linkage methods, which you can check out in the [SciPy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dendrograms\n",
    "A dendrogram is a plot that can be used to assist with visualising hierarchical clustering and determining the appropriate number of clusters. It has the initial clusters on one axis (i.e., every data point) while the other axis shows dissimilarity (for example, the Euclidean distance between the clusters).\n",
    "   \n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-dendrogram.png?raw=true\" alt=\"Dendrogram\" style=\"width: 80%;\"/>\n",
    "\n",
    "#### An example\n",
    "We start with the points that are the closest together as they join lowest down on the dissimilarity scale; these are known as the *leaves* of the dendrogram. In our example above, the following pairs of points are the closest and approximately equidistant, thus forming the new clusters: B & C, E & F, H & I, and J & K. The next closest point is A, which is closest to the cluster containing B & C, so it joins that cluster and so on and this process continues until all points are within a single cluster, as seen above on the left. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing an appropriate number of clusters\n",
    "To determine the ideal number of clusters using the dendrogram, we look for the most significant vertical distance that would not be intersected if the perpendicular lines to the dissimilarity axis were extended. The figure below might help clarify this.   \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-dendrogram-2.png?raw=true\" alt=\"Dendrogram\" style=\"width: 50%;\"/>  \n",
    "\n",
    "The lines perpendicular to the dissimilarity axis have been extended as dashed lines, and the red arrow indicates the maximum uninterrupted distance parallel to the dissimilarity axis. We can determine the ideal number of clusters by looking at how many lines run parallel to the dissimilarity axis in this area (which is two as there are two lines parallel to the dissimilarity axis within this area)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering in Python\n",
    "We will now perform hierarchical clustering on generated toy data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the data\n",
    "Let's import the libraries we will need and go ahead and generate some data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bv12xz-ZjaLD"
   },
   "outputs": [],
   "source": [
    "# data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 8 blobs in 2D space\n",
    "n_features = 2\n",
    "centers = 8\n",
    "\n",
    "X, y = make_blobs(n_samples=1000, centers=centers, n_features=n_features, random_state=68) #rand = 8, 42\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X, columns=[*[f'feature_{i}' for i in range(n_features)]])\n",
    "\n",
    "# plot data\n",
    "plt.figure(dpi=120)\n",
    "\n",
    "x1 = df['feature_0']\n",
    "x2 = df['feature_1']\n",
    "plt.scatter(x1, x2)\n",
    "\n",
    "plt.title(\"Figure 1\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling of features\n",
    "Since we are dealing with a distance measure between points, we must scale our data before computing dissimilarities. Without scaling, the clustering results will most likely depend on the feature(s) with the largest measurement scale.\n",
    "\n",
    "We will create a new 2D array called X_scaled containing the scaled features to profile the clusters in their original units of measure later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JFWpb1FXjaL3",
    "outputId": "002063f2-4d23-47ee-967e-406ec2a77667"
   },
   "outputs": [],
   "source": [
    "# create scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale the data\n",
    "X_scaled = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.histplot(df.iloc[:,0])\n",
    "plt.title('Unscaled')\n",
    "plt.gca().set_aspect('auto', adjustable='box')  # Adjust aspect ratio\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.histplot(df.iloc[:,1])\n",
    "plt.title('Unscaled')\n",
    "plt.gca().set_aspect('auto', adjustable='box')  # Adjust aspect ratio\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.histplot(X_scaled[:,0])\n",
    "plt.title('Scaled')\n",
    "plt.gca().set_aspect('auto', adjustable='box')  # Adjust aspect ratio\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.histplot(X_scaled[:,1])\n",
    "plt.title('Scaled')\n",
    "plt.gca().set_aspect('auto', adjustable='box')  # Adjust aspect ratio\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dendrogram\n",
    "Now that we have chosen our hyperparameters, we can create our dendrogram by performing hierarchical clustering on our scaled features. Our strategy here is as follows:\n",
    "\n",
    "1. Generate and visualise the dendrogram using the SciPy library.\n",
    "2. Use a dendrogram to determine the optimal number of clusters.\n",
    "3. Use the number of clusters to fit `sklearn`'s agglomerative hierarchical algorithm to the data.\n",
    "\n",
    "You are encouraged to try different distance metrics and linkage methods and see how your results differ.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform hierarchical clustering in SciPy using the `linkage` function from the `scipy.cluster.hierarchy` module. This function returns a linkage matrix (our hierarchical clustering encoded as an array). It is at this point that we convert this linkage matrix into our dendrogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to view linkage function docstring\n",
    "# help(sch.linkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering\n",
    "linkage_matrix = sch.linkage(X_scaled, method='average', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot dendrogram\n",
    "plt.figure(figsize=(10,6))\n",
    "dendrogram = sch.dendrogram(linkage_matrix)#, no_plot=True) # no_plot because we want to plot manually\n",
    "\n",
    "# The linkage matrix also contains the dissimilarity measures associated with each merge we've done\n",
    "# We can use these to plot a few horizontal lines on the dendrogram\n",
    "# that can help us find the right number of clusters better\n",
    "z = linkage_matrix[:,2]\n",
    "\n",
    "for i in range(1,16):\n",
    "    rng = [z[-i],z[-i]]\n",
    "    dom = [0,40000]\n",
    "    plt.plot(dom,rng,'black', alpha=0.3)\n",
    "\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Datapoints')\n",
    "plt.ylabel('Dissimilarity')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dendrogram records which clusters were merged at each iteration and the resulting increase in total dissimilarity. Initially, all observations began as their own clusters (0 on the y-axis) and were iteratively merged until all observations belonged to the same cluster (top of the y-axis).      \n",
    "\n",
    "We choose our optimal number of clusters by looking for the point at which merging the closest two clusters results in the most significant increase in dissimilarity. The Scipy plotting method helps us out a bit here by indicating the optimal number of clusters with colours.  We can see that the most significant increase in dissimilarity occurs when we merge from 4 into 3 clusters, so we will choose to use 4 clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above, it looks like our chosen hyperparameters `linkage='average' ` and `metric='euclidean'` could not detect the smaller clusters. The dataset has 8 clusters, but we are only able to pick up the 4 that have the best separation from each other. In other words, clusters that were too close to each other have been merged.\n",
    "\n",
    "In your own time, develop a hyperparameter configuration that recognises all 8 clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kdxb8g8ex50K"
   },
   "source": [
    "## Agglomerative hierarchical clustering in Sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3RNVdhg0x50O"
   },
   "source": [
    "To implement Agglomerative Hierarchical Clustering in Sklearn, we have to do the following:\n",
    "\n",
    "* Import the AgglomerativeClustering class;\n",
    "* Create an instance of the `AgglomerativeClustering` class, specifying the number of clusters and\n",
    "* Use the `fit()` method to find the specified number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4\n",
    "hc = AgglomerativeClustering(n_clusters=K, linkage='average', metric='euclidean').fit(X_scaled)\n",
    "df['cluster_label']  = hc.labels_ \n",
    "df['cluster_label'] = df['cluster_label'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise how well we did with $K=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.figure(dpi=120)\n",
    "for k in range(K):\n",
    "    x1 = df[df['cluster_label'] == k]['feature_0']\n",
    "    x2 = df[df['cluster_label'] == k]['feature_1']\n",
    "    plt.scatter(x1, x2, label=\"k = \"+str(k+1))\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"K = 4\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster profiling\n",
    "The next step is to explore the results of clustering. Can we identify any patterns or trends in the four clusters?   \n",
    "\n",
    "Let's start by aggregating the clusters by their median feature values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('cluster_label').agg('median').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also look at the size of each cluster by counting the data points it contains\n",
    "df.groupby(\n",
    "    'cluster_label').count().reset_index().plot(kind='bar', \n",
    "                                              x='cluster_label', \n",
    "                                              figsize=(8,6), \n",
    "                                              title='Number of data points per Cluster')\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.ylabel(\"# of data points\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TaOoFDdOx50j"
   },
   "source": [
    "## Advantages & disadvantages of agglomerative hierarchical clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PCZfZYzx50k"
   },
   "source": [
    "**Advantages**\n",
    "\n",
    "* Simple to implement;\n",
    "* No prior information about the number of clusters in the data is required.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "* The algorithm takes much longer to run compared to K-Means clustering, worsening as the size of the dataset increases;\n",
    "* Determining an ideal number of clusters can be difficult by looking at the dendrogram."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have now learned how to use hierarchical clustering. We covered:\n",
    "\n",
    "- Agglomerative vs divisive hierarchical clustering;\n",
    "- Different ways of choosing how cluster dissimilarity is computed and\n",
    "- An implementation of hierarchical clustering, which uses both sklearn and SciPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix \n",
    "Links to additional resources to help with the understanding of concepts presented in the train:\n",
    "\n",
    "- [Hierarchical clustering distance measures](https://youtu.be/VMyXc3SiEqs?list=PLBv09BD7ez_7qIbBhyQDr-LAKWUeycZtx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
