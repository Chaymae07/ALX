{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k52b0R_HjaKt"
   },
   "source": [
    "# Examples: K-means clustering\n",
    "¬© ExploreAI Academy\n",
    "\n",
    "In the train, we will explore the application of K-means clustering, an unsupervised learning technique, to group data points based on similarities in their features. We'll cover theory, implementation using sklearn, and methods for determining the optimal number of clusters, providing insights into underlying patterns within datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "By the end of this train, you should be able to:\n",
    "- Implement the K-means clustering algorithm\n",
    "- Implement a K-means clustering model in sklearn\n",
    "- Choose the optimal number of clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "**Unsupervised learning**¬†is an area of statistical learning that we use to infer trends to group cases based on similar attributes, naturally occurring trends, patterns, or relationships from unlabeled data. These models are often referred to as¬†self-organising maps. A variety of methods are used as a strategy for dividing data into groups to carry out unsupervised learning, one of which is known as¬†**clustering**. Clustering methods use data features to group observations into either distinct or overlapping sets.\n",
    "\n",
    "Let's learn more about how clustering works!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sm6oc90_x5y-"
   },
   "source": [
    "## Clustering\n",
    "Clustering is the process of grouping similar data points together, such that data points in the same groups are more similar to other data points in that group than those in other groups. \n",
    "\n",
    "The aim is to divide groups with similar characteristics and assign them to clusters. This will give us insights into the underlying patterns of the different groups/clusters. In this course, we will explore three types of clustering:\n",
    "\n",
    "* **Hard Clustering**\n",
    "* **Hierarchical Clustering**\n",
    "* **Soft/Fuzzy Clustering**\n",
    "\n",
    "Explanations are given in the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "209V9V0ox5y_"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-clustering-types.png?raw=true\" alt=\"Types of Clustering\" style=\"width: 80%;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVk7tJ9YjaK1"
   },
   "source": [
    "\n",
    "## The K-means clustering algorithm\n",
    "K-means is an example of what is known as a **hard clustering** method, which means that the clusters, or groups that result, are distinct and non-overlapping. In other words, each observation will ultimately belong to only one cluster, although it may be passed around between clusters during the iterative training of the clustering model.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-kmeans-clusters.png?raw=true\" alt=\"Types of Clustering\" style=\"width: 70%\">\n",
    "\n",
    "The main objective in K-means is to partition a dataset into K clusters. This is achieved by randomly initialising K cluster centroids (points to be used as the cluster centres), assigning each data point to the nearest centroid (according to some¬†**distance measure**), and then updating the cluster centroids as well as data point membership at each iteration until the centroids stop changing.\n",
    "\n",
    "One drawback of the K-means algorithm (and many other clustering algorithms) is that the analyst needs to specify the number of clusters, K. This is a difficult task because this number is seldom obvious for most real-world datasets. However, we do have methods available which we can use to determine the optimal number of clusters. We will cover some of these later on, but for now, let's take a look at the K-means algorithm, which, although it comes in many flavours, generally works as follows:\n",
    "\n",
    "1. Pick the number of clusters K and randomly assign each observation a cluster number from 1 to K.\n",
    "2. Repeat the following until cluster assignments stop changing:  \n",
    "    a. For each of the K clusters, compute the cluster centroid. The $k^{th}$ cluster centroid is the mean of the observations in the $k^{th}$ cluster.  \n",
    "    b. Assign each observation to the cluster whose centroid is closest (where closest is defined using a chosen distance metric such as **Euclidean distance**).\n",
    "    \n",
    "The sequence of images below illustrates the K-means algorithm as it iterates over a data set (these correspond to the algorithm above).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/clustering_algorithm.png?raw=true\" alt=\"Types of Clustering\" style=\"width: 70%\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means in sklearn\n",
    "\n",
    "Let's cement this theory by implementing, or fitting, a K-means model to some data.  \n",
    "\n",
    "### Setting up the data\n",
    "Before we dive in, let's load in our libraries and go ahead and create some toy data to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bv12xz-ZjaLD"
   },
   "outputs": [],
   "source": [
    "# data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 8 blobs in 2D space\n",
    "n_features = 2\n",
    "centers = 8\n",
    "\n",
    "X, y = make_blobs(n_samples=1000, centers=centers, n_features=n_features, random_state=68) #rand = 8, 42\n",
    "\n",
    "data = np.concatenate([X, y.reshape(-1, 1)], axis=1)\n",
    "df = pd.DataFrame(data, columns=[*[f'feature_{i}' for i in range(n_features)], 'y'])\n",
    "df['y'] = df['y'].astype(int)\n",
    "\n",
    "# plot data\n",
    "plt.figure(dpi=120)\n",
    "for center in range(centers):\n",
    "    x1 = df[df['y'] == center]['feature_0']\n",
    "    x2 = df[df['y'] == center]['feature_1']\n",
    "    plt.scatter(x1, x2, label=str(center))\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Figure 1\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've successfully created a dataset consisting of two features and a few distinct clusters (indicated by different colours in the plot shown above). These colours act as a sort of label, i.e., for any pair of features $X$, we know what the corresponding label $y$ (the colour or cluster number) is. As such, this means that we have a labelled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the data look like?\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in unsupervised learning, we do not have access to these labels. As such, let's pretend we don't know the number of clusters (or their locations) and pretend that we just came across the following dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same data with the y column dropped\n",
    "df_no_labels = df.drop('y',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=120)\n",
    "x1 = df_no_labels['feature_0']\n",
    "x2 = df_no_labels['feature_1']\n",
    "plt.scatter(x1, x2)\n",
    "plt.title(\"Figure 2\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with two problems to solve:\n",
    "\n",
    "1. We don't know which pair of features, $X$, belong to which cluster.\n",
    "\n",
    "2. We don't know how many clusters, $K$, there are in the dataset. \n",
    "\n",
    "\n",
    "We usually address the first problem by building a clustering machine learning model, and we address the second problem by making an educated guess of how many clusters there are. As you will notice with the K-means algorithm, many clustering algorithms require that we first select the number of clusters¬†ùêæ¬†before training the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fy0vRz5ZjaL1"
   },
   "source": [
    "Since clustering algorithms rely on computing some form of distance metric, it is good practice to scale the data. If we don't do this, then we risk having features that contribute way more than others when deciding which cluster a given data point belongs to. For example, let's suppose we had a dataset with two features: `age` (ranging from `18` to `60`) and `income` (ranging from `10000` to `45000`). If we attempt to cluster without rescaling or normalising, then the `income` feature will always dominate the distance calculation to the extent that our results would be the same as if we didn't have the `age` feature in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JFWpb1FXjaL3",
    "outputId": "002063f2-4d23-47ee-967e-406ec2a77667"
   },
   "outputs": [],
   "source": [
    "# create scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale the data\n",
    "X_scaled = scaler.fit_transform(df_no_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding on K\n",
    "\n",
    "With clustering, we can't be too sure how many clusters we will get, and what information they will be clustered on! Looking at Figure 2 above, it seems that we have 4 or 5 distinct clusters. Let's pick $K=4$ for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "akzczS8QjaLB"
   },
   "source": [
    "### Fitting the K-means model\n",
    "\n",
    "Finally, apply Sklearn's `KMeans` model to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans model\n",
    "from sklearn.cluster import KMeans\n",
    "# Import time to measure algorithm runtime \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=K, random_state=42)\n",
    "km.fit(X_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have only picked two hyperparameters in this case: `n_clusters` (K) and `random_state` (the algorithm has random elements, and this helps ensure that we can reproduce our results). However, there are other useful hyperparameters such as `max_iter`, `tol`, etc. You can find out what each of these is for by using the `help()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(KMeans)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating k-means\n",
    "\n",
    "Notice the useful method `.cluster_centers_` on the `KMeans` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain cluster memberships for each item in the data\n",
    "y_preds = km.predict(X_scaled)\n",
    "df_no_labels['cluster_label'] = y_preds\n",
    "centers = scaler.inverse_transform(km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise how well we did with $K=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.figure(dpi=120)\n",
    "for k in range(K):\n",
    "    x1 = df_no_labels[df_no_labels['cluster_label'] == k]['feature_0']\n",
    "    x2 = df_no_labels[df_no_labels['cluster_label'] == k]['feature_1']\n",
    "    plt.scatter(x1, x2, label=\"k = \"+str(k+1))\n",
    "    \n",
    "# show cluster centroid locations\n",
    "plt.scatter(centers[:,0],centers[:,1],label=\"centroid\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"K = 4\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although K-means tried, this configuration is a far cry from the initial clusters we generated in Figure 1. A useful observation here is that cluster centroids can also occur in places where there is no data. \n",
    "\n",
    "Anyway, let's see if we can pick a better K.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJzvvvz_jaK5"
   },
   "source": [
    "## Deciding on the number of Clusters (K)\n",
    "We can now revisit the topic of having to specify a number of clusters K when using the K-means method. \n",
    "\n",
    "### 1. Within cluster variation \n",
    "We will make use of a popular technique called the¬†**'Elbow Method'**¬†to determine how many clusters to choose. \n",
    "\n",
    "To use the elbow method, we will perform K-means clustering a number of times, changing the value of k each time. For each value of k, we will measure the **total within-cluster sum of squares**, or **WCSS**. Put simply, the WCSS is computed by calculating the sum of squared distances between cluster centroids and the data points that belong to them. These *within-cluster sum of squares* are then added together to obtain the *total within-cluster sum of squares* or *WCSS*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ \\Large\n",
    "\\text{WCSS} = \\sum \\limits_{i=1}^{K} \\sum \\limits_{j = 1}^{|P_i|} \\text{distance}(P_{ij},C_i)^{2}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NKqfyFTjaK-"
   },
   "source": [
    "WCSS is a measure of the similarity of the observations within our clusters. Lower WCSS means that a cluster is more tightly grouped. Naturally, as we increase k, WCSS will decrease until k is equal to the number of data points in our data set (when WCSS will be zero). But what use is it to have as many clusters as there are observations? We need to find the optimal number of clusters so that each cluster is sufficiently consistent in its attributes without having too many clusters.   \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-wcss.png?raw=true\" alt=\"Types of Clustering\" style=\"width: 80%;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_KyT0u7jaMD"
   },
   "source": [
    "Take a look at the loop below.\n",
    "\n",
    "We have chosen a range of values for k between 1 and 20. For each value of k, we will run the k-means clustering algorithm and record the WCSS (referred to as **`inertia_`** in sklearn. However, for reasons that will become obvious later, we will implement our own WCSS here).\n",
    "\n",
    "_Have a look at the docstring of the **`KMeans`** function (using `help()`)to find out what the **`n_init`** and **`max_iter`** arguments do._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually implement the WCSS\n",
    "def within_cluster_variation(df, label_col='label'):\n",
    "    centroids = df.groupby(label_col).mean()\n",
    "    out = 0\n",
    "    for label, point in centroids.iterrows():\n",
    "        df_features = df[df[label_col] == label].drop(label_col, axis=1)\n",
    "        out += (df_features - point).pow(2).sum(axis=1).sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u24ThsH5jaMI"
   },
   "outputs": [],
   "source": [
    "# let's try everything between 2 and 20 clusters\n",
    "n_clusters = np.arange(2, 21)\n",
    "\n",
    "# store errors for each value of k\n",
    "errors = []\n",
    "\n",
    "# for i between 2 and 20\n",
    "for k in n_clusters:\n",
    "    # perform k-means clustering\n",
    "    km = KMeans(n_clusters=k, n_init=10, max_iter=300, random_state=42)\n",
    "    km.fit(X_scaled)\n",
    "    \n",
    "    # measure BCSS\n",
    "\n",
    "    y_preds = km.predict(X_scaled)\n",
    "    df_no_labels['cluster_label'] = y_preds\n",
    "    errors.append(within_cluster_variation(df_no_labels,'cluster_label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQV8cik7jaMP"
   },
   "source": [
    "Now that we have performed k-means clustering for each value of k between 2 and 20, we need to create a plot showing the WCSS for each corresponding value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jSHzvH5jaMQ",
    "outputId": "32f35fc5-2673-4a83-e9a0-ef2e673fccd2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method for Determining Optimal Value of k')\n",
    "plt.scatter(n_clusters, errors)\n",
    "plt.plot(n_clusters, errors)\n",
    "plt.xticks(n_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnY2VsN1jaMV"
   },
   "source": [
    "Using the elbow method is as simple as observing the plot of the within-cluster sum of squares (WCSS) for each number of clusters and deciding where the 'elbow' is. The elbow is loosely defined as the point at which the WCSS curve has its greatest decrease in slope. In other words, it is the point after which adding another cluster does not meaningfully decrease the WCSS.\n",
    "\n",
    "Below, we have provided a copy of the code used to generate the elbow plot from above. The 2nd last line of code is incomplete - set the `x=` argument in the `plt.axvline()` function to draw a vertical line at the number of clusters you think the elbow occurs at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AncCLw9-jaMX",
    "outputId": "a88c53ce-4d15-478e-cc3f-a1f42265f233"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method for Determining Optimal Value of k')\n",
    "plt.scatter(n_clusters, errors)\n",
    "plt.plot(n_clusters, errors)\n",
    "plt.xticks(n_clusters)\n",
    "\n",
    "# SET THE X-VALUE BELOW TO THE NUMBER OF CLUSTERS\n",
    "# AT WHICH THE ELBOW OCCURS, e.g. plt.axvline(x=18, color='r', lw=2)\n",
    "plt.axvline(x=18, color='r', lw=2)\n",
    "plt.show()\n",
    "\n",
    "# run this code block by hitting Shift+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r40RYQTNjaMb"
   },
   "source": [
    "Elbow plots are tricky to decipher, **particularly because this graph keeps on decreasing**. However, we can see a clear kink at k=6 - this is where you should have plotted your vertical line. We will now cluster our observations using k=6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BF1i-jHjaMc",
    "outputId": "bbcfe350-a7ca-4b65-bd7f-4b4361381094"
   },
   "outputs": [],
   "source": [
    "K = 6\n",
    "# remember to set the random state for reproducibility\n",
    "km = KMeans(n_clusters=K, verbose=0, random_state=42)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X_scaled)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFbEalXHjaMm"
   },
   "outputs": [],
   "source": [
    "# obtain cluster memberships for each item in the data\n",
    "y_preds = km.predict(X_scaled)\n",
    "df_no_labels['cluster_label'] = y_preds\n",
    "centers = scaler.inverse_transform(km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the clusters\n",
    "plt.figure(dpi=120)\n",
    "for k in range(K):\n",
    "    x1 = df_no_labels[df_no_labels['cluster_label'] == k]['feature_0']\n",
    "    x2 = df_no_labels[df_no_labels['cluster_label'] == k]['feature_1']\n",
    "    plt.scatter(x1, x2, label=\"k = \"+str(k+1))\n",
    "    \n",
    "# show cluster centroid locations\n",
    "plt.scatter(centers[:,0],centers[:,1],label=\"centroid\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"K = 6\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like $K=6$ did much better than $K=4$. Let's see if other methods for selecting $K$ can do better. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Between-cluster variation\n",
    "\n",
    "If the WCSS measures the variation **within each cluster** (i.e., how spread apart the data in a cluster is from the centroid), between cluster variation measures how spread apart the clusters are from each other:\n",
    "\n",
    " $$ \\Large\n",
    "\\text{BCSS} =  \\sum \\limits_{k = 1}^{K} n_{k}(C_k - \\bar{X})^{2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $n_{k}$ is the number of data points in the $k^{th}$ cluster\n",
    "- $C_k$ is the centroid of the $k^{th}$ cluster\n",
    "- $\\bar{X}$ is the mean of all data points, i.e. $\\bar{X} = \\frac{1}{n} \\sum \\limits_{i=1}^{n}X_i$\n",
    "\n",
    "Let's implement this and compare it with the WCSS method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Between cluster variation\n",
    "def between_cluster_variation(df, label_col='label'):\n",
    "    centroids = df.groupby(label_col).mean()\n",
    "    global_mean = df.drop(label_col, axis=1).mean()\n",
    "    centroid_count = df.groupby(label_col).size()\n",
    "    centroid_to_mean_dist = (centroids - global_mean).pow(2).sum(axis=1)\n",
    "    return (centroid_count*centroid_to_mean_dist).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try everything between 2 and 20 clusters\n",
    "n_clusters = np.arange(2, 21)\n",
    "\n",
    "# store errors for each value of k\n",
    "errors = []\n",
    "\n",
    "# for i between 2 and 20\n",
    "for k in n_clusters:\n",
    "    # perform k-means clustering\n",
    "    km = KMeans(n_clusters=k, n_init=10, max_iter=300, random_state=42)\n",
    "    km.fit(X_scaled)\n",
    "    \n",
    "    # measure BCSS\n",
    "\n",
    "    y_preds = km.predict(X_scaled)\n",
    "    df_no_labels['cluster_label'] = y_preds\n",
    "    errors.append(between_cluster_variation(df_no_labels,'cluster_label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQV8cik7jaMP"
   },
   "source": [
    "Now that we have performed k-means clustering for each value of k between 4 and 20, we need to create a plot showing the BCSS for each corresponding value of k. Since we are measuring how spread out the clusters are, we prefer our BCSS to be higher for clusters that are further apart and lower for those that are close. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jSHzvH5jaMQ",
    "outputId": "32f35fc5-2673-4a83-e9a0-ef2e673fccd2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Between-Cluster Sum of Squares (BCSS)')\n",
    "plt.title('Elbow Method for Determining Optimal Value of k')\n",
    "plt.scatter(n_clusters, errors)\n",
    "plt.plot(n_clusters, errors)\n",
    "plt.xticks(n_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking the optimal number of clusters here is still the same as with WCSS; the only difference is that our elbow is now upside-down. Like before, we see a clear kink at $K=6$.   \n",
    "\n",
    "However, this method also has an issue where the BCSS keeps increasing. What we would like is something that stops increasing/decreasing after we have found the right number of clusters. \n",
    "\n",
    "Since we have already seen what the clusters look like with $K=4$, let's jump straight into the next method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The CH index\n",
    "\n",
    "Again, the ideal scenario is where our clustering assignments have a small WCSS and a large BCSS simultaneously. The CH index combines these two metrics in a clever way so that it's easy to isolate areas where the WCSS is small and the BCSS is large. For a given value of $K$, the CH index can be computed as follows: \n",
    "\n",
    " $$ \\Large\n",
    "\\text{CH(K)} =  \\frac{BCSS(K)/(K-1)}{WCSS(K)/(n-1)}\n",
    "$$\n",
    "\n",
    "Notice that this involves computing both the BCSS and the WCSS. Using the functions we've defined above, the CH index can be implemented as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ch_index(df, label_col='label'):\n",
    "    n = len(df)\n",
    "    K = df[label_col].nunique()\n",
    "    B = between_cluster_variation(df, label_col)\n",
    "    W = within_cluster_variation(df, label_col)\n",
    "    return (B / (K-1)) / (W / (n-K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try everything between 1 and 20 clusters\n",
    "n_clusters = np.arange(2, 21)\n",
    "\n",
    "# store errors for each value of k\n",
    "errors = []\n",
    "\n",
    "# for i between 1 and 20\n",
    "for k in n_clusters:\n",
    "    # perform k-means clustering\n",
    "    km = KMeans(n_clusters=k, n_init=10, max_iter=300, random_state=42)\n",
    "    km.fit(X_scaled)\n",
    "    \n",
    "    # measure CH\n",
    "\n",
    "    y_preds = km.predict(X_scaled)\n",
    "    df_no_labels['cluster_label'] = y_preds\n",
    "    errors.append(ch_index(df_no_labels,'cluster_label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('CH index')\n",
    "plt.title('Elbow Method for Determining Optimal Value of k')\n",
    "plt.scatter(n_clusters, errors)\n",
    "plt.plot(n_clusters, errors)\n",
    "plt.xticks(n_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see a very obvious peak at $K=8$ (if you still remember, this is what we used to generate the data, but the algorithm didn't know this). This metric is definitely easier to interpret. The rule of thumb here is to investigate each peak. These include $K=4$, $K=8$, $K=13$, $K=16$, and $K=19$ in cases where things aren't clear. In this case, however, it is super clear that the obvious number of clusters is $K=8$! \n",
    "\n",
    "Let's try it out and see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BF1i-jHjaMc",
    "outputId": "bbcfe350-a7ca-4b65-bd7f-4b4361381094"
   },
   "outputs": [],
   "source": [
    "K = 8\n",
    "# remember to set the random state for reproducibility\n",
    "km = KMeans(n_clusters=K, verbose=0, random_state=42)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X_scaled)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFbEalXHjaMm"
   },
   "outputs": [],
   "source": [
    "# obtain cluster memberships for each item in the data\n",
    "y_preds = km.predict(X_scaled)\n",
    "df_no_labels['cluster_label'] = y_preds\n",
    "centers = scaler.inverse_transform(km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the clusters\n",
    "plt.figure(dpi=120)\n",
    "for k in range(K):\n",
    "    x1 = df_no_labels[df_no_labels['cluster_label'] == k]['feature_0']\n",
    "    x2 = df_no_labels[df_no_labels['cluster_label'] == k]['feature_1']\n",
    "    plt.scatter(x1, x2, label=\"k = \"+str(k+1))\n",
    "    \n",
    "# show cluster centroid locations\n",
    "plt.scatter(centers[:,0],centers[:,1],label=\"centroid\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"K = 8\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks great! but for good measure, let's compare with the original dataset we generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.figure(dpi=120)\n",
    "for center in range(8):\n",
    "    x1 = df[df['y'] == center]['feature_0']\n",
    "    x2 = df[df['y'] == center]['feature_1']\n",
    "    plt.scatter(x1, x2, label=str(center))\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Original clusters\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to have captured most of the clusters. However, K-means could not correctly assign data points in regions where the clusters overlap. This is a characteristic of all **hard clustering algorithms**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "Clustering algorithms like K-means produce clusters, but how do we know if these clusters are of good quality?\n",
    "We can use evaluation metrics to **assess the quality** of our clusters. Two common metrics are the `silhouette score` and the `Davies-Bouldin index`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score\n",
    "The silhouette score is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n",
    "It ranges from -1 to 1, where a high value indicates that the object is well-matched to its own cluster and poorly matched to neighbouring clusters.\n",
    "A silhouette score close to 1 suggests that the object is appropriately assigned to its cluster, while a score near -1 indicates that it is assigned to the wrong cluster.\n",
    "In clustering, we aim for a high silhouette score, as it indicates well-separated clusters and a clear distinction between them.\n",
    "The silhouette score is computed for each sample and then averaged to obtain an overall score for the clustering.\n",
    "\n",
    "Let's calculate the Silhouette score for our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_avg = silhouette_score(X_scaled, km.labels_)\n",
    "print(\"\\nSilhouette Score:\", silhouette_avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette score of 0.576 indicates a reasonably good clustering result. It suggests that the clusters are fairly distinct and well-defined, with minimal overlap between them. Overall, it indicates a relatively successful partitioning of the data into meaningful clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Bouldin Index\n",
    "The Davies-Bouldin index is another measure of cluster validity that evaluates the average similarity between each cluster and its most similar cluster.\n",
    "It is defined as the ratio of the average intra-cluster distance to the distance between cluster centroids.\n",
    "Lower values of the Davies-Bouldin index indicate better clustering, with tight and well-separated clusters.\n",
    "A high Davies-Bouldin index suggests that clusters overlap or are poorly separated.\n",
    "Like the silhouette score, the Davies-Bouldin index is used to assess the quality of clustering solutions and guide the selection of the optimal number of clusters.\n",
    "\n",
    "Let's calculate the Davies-Bouldin index for our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "db_index = davies_bouldin_score(X_scaled, km.labels_)\n",
    "print(\"\\nDavies-Bouldin Index:\", db_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Davies-Bouldin index of 0.598 indicates a relatively good clustering result. It suggests that the clusters are reasonably well-separated, with each cluster having distinct characteristics compared to the others. While not as intuitive as the silhouette score, a lower Davies-Bouldin index still signifies a good clustering outcome, indicating that the clusters are cohesive and well-defined. Therefore, in this context, a Davies-Bouldin index of 0.598 reflects a satisfactory partitioning of the data into meaningful clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette score and Davies-Bouldin index corroborate our assessment of the clustering visualisation. While K-means successfully identified the majority of clusters, it struggled with accurate assignments for some data points. Overall, K-means demonstrates moderate effectiveness as a clustering method for our dataset. However, exploring alternative clustering methods might offer improvements in clustering accuracy and overall performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and disadvantages of k-means clustering \n",
    "\n",
    "**Advantages**\n",
    "- Computationally efficient clustering technique \n",
    "- Suitable for large datasets \n",
    "- Easy to implement and interpret \n",
    "- Easily adaptable to new datasets\n",
    "\n",
    "**Disadvantages**\n",
    "- Lacks consistency \n",
    "- Sensitive to scaling \n",
    "- Dependent on initial values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MgArG2NQjaM3"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "You have now learned what clustering means, what the different types of clustering are, how to cluster a given dataset using the k-means algorithm, how to use various methods for determining the optimal number of clusters K, and finally, how to do a basic analysis of the resulting clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix \n",
    "A link to an additional resource to help with the understanding of the concepts presented in this train. \n",
    "\n",
    "> - [How to choose the number of clusters K?](http://www2.stat.duke.edu/~rcs46/lectures_2017/10-unsupervise/10-specify-clusters.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "12_kmeans_clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
